# gleich wie heavyAugment nur mehr views h√∂here batch size und lr niedriger

# General parameters
path: "~/deep_clustering_for_wce/src/dumps/see_ai_processed_training"
val_path: "~/deep_clustering_for_wce/src/dumps/kid_validation"
img_size: 224  # Reduced from 336 to manage computational load
seed: 23
name: "MoreViewsHeavyAugment"
devices: [1, 2, 3, 4]  # Update based on available GPUs

# Training parameters
training:
  epochs: 300
  batch_size: 16
  desired_effective_batch_size: 512
  num_workers: 8  # Increased for better data loading with multiple GPUs
  learning_rate: 0.001
  weight_decay: 0.04
  optimizer: "adamw"
  precision: 16
  scheduler:
    name: "cosine"
    warmup_epochs: 10
  early_stopping:
    monitor: "val_loss"
    patience: 10
    mode: "min"

# Momentum schedule for teacher model
momentum_schedule:
  base_momentum: 0.996
  final_momentum: 1.0

# DINO loss parameters
dino_loss:
  output_dim: 4096
  hidden_dim: 2048
  input_dim: 768
  bottleneck_dim: 256
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 10  # Shorter warm-up due to domain similarity
  student_temp: 0.1
  center_momentum: 0.9

# Transform parameters
transforms:
  global_crop_size: 224
  global_crop_scale: [0.5, 1.0]
  local_crop_size: 160  # Increased resolution for local crops
  local_crop_scale: [0.3, 0.5]
  n_local_views: 6
  hf_prob: 0.7
  vf_prob: 0.7
  rr_prob: 0.7
  cj_prob: 0.8
  cj_strength: 0.5
  cj_bright: 0.4
  cj_contrast: 0.4
  cj_sat: 0.0
  cj_hue: 0.0
  random_gray_scale: 0.3
  gaussian_blur: [1.0, 0.3, 0.0]  # Blur only between global views, keep local details
  sigmas: [0.1, 2.0]
  solarization_prob: 0.0
  normalize:
    mean: [0.5929, 0.3667, 0.1843]
    std: [0.1932, 0.1411, 0.0940]