{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been merged and saved into directories per Sigma.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_model_evaluation_csvs(base_path):\n",
    "    \"\"\"\n",
    "    Merge all CSV files within each model's Sigma directory across all datasets,\n",
    "    aggregating per sigma and keeping models separate.\n",
    "\n",
    "    :param base_path: Path to the Evaluation directory containing dataset directories.\n",
    "    :return: Dictionary with (model, sigma) tuples as keys and concatenated DataFrames as values.\n",
    "    \"\"\"\n",
    "    evaluation_data = {}\n",
    "\n",
    "    # Iterate through each dataset directory\n",
    "    for dataset_dir in os.listdir(base_path):\n",
    "        dataset_path = os.path.join(base_path, dataset_dir)\n",
    "\n",
    "        if os.path.isdir(dataset_path):  # Ensure it's a directory (e.g., ncm_12)\n",
    "            # Inside dataset directory, look for model directories (e.g., resnet, sigma1)\n",
    "            for model_dir in os.listdir(dataset_path):\n",
    "                model_path = os.path.join(dataset_path, model_dir)\n",
    "\n",
    "                if os.path.isdir(model_path):  # Ensure it's a directory (model directory)\n",
    "                    # Look for subdirectories starting with 'Sigma'\n",
    "                    sigma_dirs = [d for d in os.listdir(model_path) if d.startswith('Sigma')]\n",
    "\n",
    "                    for sigma_dir in sigma_dirs:\n",
    "                        sigma_path = os.path.join(model_path, sigma_dir)\n",
    "\n",
    "                        if os.path.isdir(sigma_path):\n",
    "                            # Initialize list for this model and sigma\n",
    "                            key = (model_dir, sigma_dir)\n",
    "                            if key not in evaluation_data:\n",
    "                                evaluation_data[key] = []\n",
    "\n",
    "                            # Inside Sigma directory, look for CSV files\n",
    "                            csv_files = [f for f in os.listdir(sigma_path) if f.endswith('.csv')]\n",
    "\n",
    "                            for csv_file in csv_files:\n",
    "                                file_path = os.path.join(sigma_path, csv_file)\n",
    "\n",
    "                                # Read the CSV file\n",
    "                                df = pd.read_csv(file_path)\n",
    "\n",
    "                                # Add columns for 'Dataset', 'Model', 'Sigma'\n",
    "                                df['Dataset'] = dataset_dir\n",
    "                                df['Model'] = model_dir\n",
    "                                df['Sigma'] = sigma_dir\n",
    "\n",
    "                                # Store the data in the evaluation_data dictionary under the (model, sigma) key\n",
    "                                evaluation_data[key].append(df)\n",
    "\n",
    "    # Concatenate all the CSV files for each (model, sigma)\n",
    "    combined_data = {}\n",
    "    for key, dfs in evaluation_data.items():\n",
    "        combined_data[key] = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Example usage\n",
    "base_evaluation_path = \"./dumps/Evaluation/\"\n",
    "merged_results = merge_model_evaluation_csvs(base_evaluation_path)\n",
    "\n",
    "# Save the combined results into separate files for each model and sigma\n",
    "for (model, sigma), combined_df in merged_results.items():\n",
    "    # Clean the sigma name to remove any file system incompatible characters if necessary\n",
    "    safe_sigma = sigma.replace('/', '_').replace('\\\\', '_')\n",
    "    # Create directory for this Sigma if it doesn't exist\n",
    "    sigma_dir = os.path.join('.', safe_sigma)\n",
    "    os.makedirs(sigma_dir, exist_ok=True)\n",
    "    # Build the output path\n",
    "    output_path = os.path.join(sigma_dir, f\"combined_{model}_{safe_sigma}_results.csv\")\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"CSV files have been merged and saved into directories per Sigma.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Best Performances + Generate Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing best metric: accuracy_with_anomalies\n",
      "The best performances have been saved to ./best_performances_accuracy_with_anomalies.csv\n",
      "LaTeX table generated and saved to ./performance_comparison_table_accuracy_with_anomalies.tex\n",
      "\n",
      "Processing best metric: nmi_with_anomalies\n",
      "The best performances have been saved to ./best_performances_nmi_with_anomalies.csv\n",
      "LaTeX table generated and saved to ./performance_comparison_table_nmi_with_anomalies.tex\n",
      "\n",
      "Processing best metric: ari_with_anomalies\n",
      "The best performances have been saved to ./best_performances_ari_with_anomalies.csv\n",
      "LaTeX table generated and saved to ./performance_comparison_table_ari_with_anomalies.tex\n",
      "\n",
      "All tables have been processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def collect_best_performances(experiments_root, metrics, best_metric='accuracy_with_anomalies'):\n",
    "    \"\"\"\n",
    "    Collects the best performance across different minCL and Sigma values for each model,\n",
    "    and returns a DataFrame with one column per model, including all specified metrics.\n",
    "\n",
    "    :param experiments_root: Path to the root directory containing ExperimentsMinCl* directories.\n",
    "    :param metrics: A list of performance metrics to include.\n",
    "    :param best_metric: The metric used to select the best performance per model.\n",
    "    :return: DataFrame containing the best performances, with models as columns.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Iterate through each ExperimentsMinCl* directory\n",
    "    for mincl_dir in os.listdir(experiments_root):\n",
    "        mincl_path = os.path.join(experiments_root, mincl_dir)\n",
    "        if os.path.isdir(mincl_path) and mincl_dir.startswith('ExperimentsMinCl'):\n",
    "            # Extract minCL value from directory name\n",
    "            minCL_value = int(mincl_dir.replace('ExperimentsMinCl', ''))\n",
    "\n",
    "            # Iterate through Sigma directories\n",
    "            for sigma_dir in os.listdir(mincl_path):\n",
    "                sigma_path = os.path.join(mincl_path, sigma_dir)\n",
    "                if os.path.isdir(sigma_path):\n",
    "                    # Iterate through CSV files in the Sigma directory\n",
    "                    for csv_file in os.listdir(sigma_path):\n",
    "                        if csv_file.endswith('.csv'):\n",
    "                            csv_path = os.path.join(sigma_path, csv_file)\n",
    "\n",
    "                            # Read the CSV file\n",
    "                            df = pd.read_csv(csv_path)\n",
    "\n",
    "                            # Add columns for 'minCL', 'Sigma', 'Model' if not present\n",
    "                            df['minCL'] = minCL_value\n",
    "                            df['Sigma'] = sigma_dir\n",
    "\n",
    "                            # Extract model name from file name if 'Model' column not in df\n",
    "                            if 'Model' not in df.columns:\n",
    "                                # Expected filename format: 'combined_{Model}_{Sigma}_results.csv'\n",
    "                                model_name = csv_file.replace('combined_', '').replace('_results.csv', '')\n",
    "                                # Remove '_SigmaX' from model_name\n",
    "                                model_name = model_name.replace(f'_{sigma_dir}', '')\n",
    "                                df['Model'] = model_name\n",
    "\n",
    "                            # Append to the list\n",
    "                            all_data.append(df)\n",
    "\n",
    "    # Combine all data into a single DataFrame\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Check if the metrics exist in the DataFrame\n",
    "    for metric in metrics:\n",
    "        if metric not in combined_df.columns:\n",
    "            raise ValueError(f\"The specified metric '{metric}' is not found in the data.\")\n",
    "\n",
    "    # Check if the best_metric exists in the DataFrame\n",
    "    if best_metric not in combined_df.columns:\n",
    "        raise ValueError(f\"The specified best_metric '{best_metric}' is not found in the data.\")\n",
    "\n",
    "    # Group by 'Dataset' if it exists, otherwise proceed without it\n",
    "    if 'Dataset' in combined_df.columns:\n",
    "        group_columns = ['Dataset']\n",
    "    else:\n",
    "        group_columns = []\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    # For each group (Dataset or overall), find the best performance per model\n",
    "    for _, group_df in combined_df.groupby(group_columns):\n",
    "        # For each model, find the row with the best performance based on the best_metric\n",
    "        best_performance_per_model = group_df.loc[group_df.groupby('Model')[best_metric].idxmax()]\n",
    "\n",
    "        # Pivot the DataFrame to have models as columns\n",
    "        pivot_df = best_performance_per_model.pivot(\n",
    "            index=group_columns, columns='Model', values=metrics + ['Sigma', 'minCL']\n",
    "        )\n",
    "\n",
    "        # Flatten the MultiIndex columns\n",
    "        pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]\n",
    "\n",
    "        # Reset index to turn index into columns\n",
    "        pivot_df = pivot_df.reset_index()\n",
    "\n",
    "        # Append to results\n",
    "        results.append(pivot_df)\n",
    "\n",
    "    # Concatenate all results\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def generate_latex_table(csv_file, metrics):\n",
    "    \"\"\"\n",
    "    Generates LaTeX code for a table comparing EndoFM and ResNet101 across datasets.\n",
    "\n",
    "    :param csv_file: Path to the CSV file containing the data.\n",
    "    :param metrics: A list of performance metrics to include in the table.\n",
    "    :return: LaTeX code as a string.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Adjust the model names in the column headers to match those in the CSV\n",
    "    df.columns = df.columns.str.replace('ENDO_FM', 'EndoFM')\n",
    "    df.columns = df.columns.str.replace('RES_NET_101', 'ResNet101')\n",
    "\n",
    "    # List of models to include in the table\n",
    "    models = ['EndoFM', 'ResNet101']\n",
    "\n",
    "    # Adjust dataset names to remove underscores and hyphens (e.g., 'ncm_1' to 'ncm1')\n",
    "    df['Dataset'] = df['Dataset'].str.replace('_', '').str.replace('-', '')\n",
    "\n",
    "    # Create a function to extract numerical part for sorting\n",
    "    def dataset_sort_key(name):\n",
    "        # Match 'ncm' or 'ncmrmv' followed by numbers\n",
    "        m = re.match(r'(ncm)(rmv)?(\\d+)', name)\n",
    "        if m:\n",
    "            prefix = m.group(1)\n",
    "            rmv = m.group(2)  # 'rmv' or None\n",
    "            num = int(m.group(3))\n",
    "            # Assign a number for sorting\n",
    "            # 'ncm' datasets first, then 'ncmrmv'\n",
    "            prefix_order = {'ncm': 0, 'ncmrmv': 1}\n",
    "            rmv_part = 'rmv' if rmv else ''\n",
    "            prefix_key = prefix + rmv_part\n",
    "            order = prefix_order.get(prefix_key, 2)\n",
    "            return (order, num)\n",
    "        else:\n",
    "            return (2, name)  # Place unknown datasets at the end\n",
    "\n",
    "    # Extract datasets\n",
    "    datasets = df['Dataset'].unique()\n",
    "\n",
    "    # Sort datasets according to the desired order\n",
    "    sorted_datasets = sorted(datasets, key=dataset_sort_key)\n",
    "\n",
    "    # Prepare the LaTeX table header with dynamic metrics\n",
    "    columns = ' & '.join(['Accuracy', 'NMI', 'ARI', 'Sigma', 'MinCL'])\n",
    "    latex_table = r'''\\begin{table}[h]\n",
    "\\small\n",
    "\\setlength\\tabcolsep{3pt}\n",
    "    \\centering\n",
    "    \\vspace{-0.05in}\n",
    "    \\begin{tabular}{cllllll}\n",
    "    \\toprule\n",
    "        Dataset & Backbone & %s \\\\\n",
    "        \\midrule\n",
    "''' % columns\n",
    "\n",
    "    # For each dataset in sorted order\n",
    "    for dataset in sorted_datasets:\n",
    "        # Get the rows corresponding to this dataset\n",
    "        dataset_rows = df[df['Dataset'] == dataset]\n",
    "        if dataset_rows.empty:\n",
    "            continue\n",
    "\n",
    "        # Add the dataset name in the first column, merged over the number of models\n",
    "        latex_table += r'    \\multirow{%d}{*}{%s}' % (len(models), dataset) + '\\n'\n",
    "\n",
    "        for idx, model in enumerate(models):\n",
    "            row = dataset_rows.iloc[0]  # Adjusted to get the correct row per model\n",
    "            metric_values = []\n",
    "            for metric in metrics:\n",
    "                # Attempt to retrieve the metric value\n",
    "                try:\n",
    "                    value = row[f'{metric}_{model}']\n",
    "                except KeyError:\n",
    "                    value = 'N/A'\n",
    "                if isinstance(value, (int, float)):\n",
    "                    value_formatted = f'{value:.2f}'\n",
    "                else:\n",
    "                    value_formatted = value\n",
    "                metric_values.append(value_formatted)\n",
    "\n",
    "            # Process Sigma to extract numerical value\n",
    "            sigma_value = row[f'Sigma_{model}']\n",
    "            # Assuming Sigma is like 'Sigma3', extract the number\n",
    "            sigma_num = re.findall(r'\\d+', str(sigma_value))\n",
    "            if sigma_num:\n",
    "                sigma_value = float(sigma_num[0])\n",
    "            else:\n",
    "                sigma_value = 'N/A'\n",
    "\n",
    "            minCL_value = row[f'minCL_{model}']\n",
    "\n",
    "            # Add the Backbone and metric values\n",
    "            if idx > 0:\n",
    "                latex_table += r'    '  # Indent for alignment\n",
    "            latex_table += r' & %s & %s & %s & %s & %.1f & %s \\\\' % (\n",
    "                model,\n",
    "                metric_values[0],\n",
    "                metric_values[1],\n",
    "                metric_values[2],\n",
    "                sigma_value if isinstance(sigma_value, float) else sigma_value,\n",
    "                minCL_value\n",
    "            ) + '\\n'\n",
    "        latex_table += r'    \\midrule' + '\\n'\n",
    "\n",
    "    # Close the LaTeX table\n",
    "    latex_table += r'''    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\vspace{-0.1in}\n",
    "    \\caption{Comparison of EndoFM and ResNet101 across datasets.}\n",
    "    \\label{tab:performance_comparison}\n",
    "\\end{table}\n",
    "'''\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "def save_latex_table(csv_file, metrics, output_tex_file):\n",
    "    \"\"\"\n",
    "    Saves only the LaTeX table to a .tex file, ready for inclusion in an existing LaTeX document.\n",
    "\n",
    "    :param csv_file: Path to the CSV file containing the data.\n",
    "    :param metrics: A list of performance metrics to include in the table.\n",
    "    :param output_tex_file: Path to save the generated .tex file.\n",
    "    \"\"\"\n",
    "    # Generate the LaTeX table\n",
    "    latex_table = generate_latex_table(csv_file, metrics)\n",
    "\n",
    "    # Write the LaTeX table to the output file\n",
    "    with open(output_tex_file, 'w') as tex_file:\n",
    "        tex_file.write(latex_table)\n",
    "\n",
    "    print(f\"LaTeX table generated and saved to {output_tex_file}\")\n",
    "\n",
    "\n",
    "\n",
    "experiments_root = './Experiments'  # Path to the 'Experiments' directory\n",
    "metrics_to_include = ['accuracy_with_anomalies', 'nmi_with_anomalies', 'ari_with_anomalies']  # List of metrics to include\n",
    "best_metrics = ['accuracy_with_anomalies', 'nmi_with_anomalies', 'ari_with_anomalies']  # Metrics to use for selecting best performances\n",
    "\n",
    "for best_metric in best_metrics:\n",
    "    print(f\"\\nProcessing best metric: {best_metric}\")\n",
    "\n",
    "    # Collect best performances based on the current best_metric\n",
    "    try:\n",
    "        best_results_df = collect_best_performances(experiments_root, metrics=metrics_to_include, best_metric=best_metric)\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting best performances for {best_metric}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Define output CSV and LaTeX filenames\n",
    "    safe_metric = best_metric.replace('/', '_').replace('\\\\', '_')\n",
    "    output_csv_path = f'./best_performances_{safe_metric}.csv'\n",
    "    output_tex_file = f'./performance_comparison_table_{safe_metric}.tex'\n",
    "\n",
    "    # Save the best performances to a CSV file\n",
    "    try:\n",
    "        best_results_df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"The best performances have been saved to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV for {best_metric}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Generate and save the LaTeX table\n",
    "    try:\n",
    "        save_latex_table(csv_file=output_csv_path, metrics=metrics_to_include, output_tex_file=output_tex_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating LaTeX table for {best_metric}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nAll tables have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b-thesis-FC_XzYgg-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
